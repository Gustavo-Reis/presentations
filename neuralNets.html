<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">

		<title>Artificial Intelligence Applied to Games - Introduction to AI</title>

		<meta name="description" content="Nuts and Bolts of AI and Machine Learning">
		<meta name="author" content="Gustavo Reis">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/obsidian.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>				
					<section>
						<h1>Nuts and Bolts of AI and Machine Learning</h1>
					</section>
					<section>
						<h2>Buzz Words</h2>
						<p class="fragment">AI - Artificial Intelligence</p>
						<p class="fragment">Machine Learning</p>
					</section>
				</section>
				<section>
					<section>
						<h2>Artificial Intelligence</h2>
					</section>
					<section>
						<h1>Approaches to AI</h1>
						<table>
							<tr>
								<th style="border:0;">&nbsp;</th>
								<th class="fragment" style="border:0;text-align:center;">Human</th>
								<th class="fragment" style="border:0;text-align:center;">Rational</th>
							</tr>
							<tr>
								<th class="fragment" style="border:0;vertical-align:middle">Think</th>
								<td class="fragment" style="border:1px solid black;border-top:0;border-left:0;">The exciting new effort to make computers think…<br>(Haugeland 1985)</td>
								<td class="fragment">The study of computations that make it possible to perceive, reason and act<br>(Winston 1992)</td>
							</tr>
							<tr>
								<th class="fragment" style="vertical-align:middle">Act</th>
								<td class="fragment" style="border-right:1px solid black;">The study of how to make computers do things at which at the moment people are better<br>(Rich & Knight 1992)</td>
								<td class="fragment">The study and construction of rational agents<br>(Russell & Norvig 1995)</td>
							</tr>
						</table>
						<p class="fragment">"By distinguishing between <span style="color:blue"><i>human</i></span> and <b><i>rational</i></b> <span style="color:#FF0000">behavior</span>, we are not suggesting that humans are necessarily "irrational" in the sense of "emotionally unstable" or "insane". <span style="color:green">One merely need note that we are not perfect</span>: not all chess players are grandmasters, and, unfortunately, not everyone gets an A on the exam."</p>
					</section>
				</section>
				<section>
					<section>
						<h1>"Zipped" history of AI</h1>
						<ul>
							<li class="fragment">1943 - 1956: Gestation and birth</li>
							<ul>
								<li class="fragment">W. Macculloch and W. Pitts proposed a model of artificial neurons</li>
								<li class="fragment">C. Shannon and A. Turing were already writing programs to play chess (even without access to computers…)</li>
								<li class="fragment">During the 1950s, A. Newell and H. Simon developed a program capable of proving mathematical theorems</li>
								<li class="fragment">In 1956 there was a workshop where the name “Artificial Intelligence” was born</li>
							</ul>
						</ul>
					</section>
					<section>
						<h1>"Zipped" history of AI</h1>
						<ul>
							<li class="fragment">1952 - 1969: Early enthusiasm, great expectations</li>
							<ul>
								<li class="fragment">In this period, many problems were solved whose resolution by computers was considered to be impossible</li>
								<li class="fragment">A. Samuel developed a program that learned to play checkers at strong amateur level (being able to beat him also)</li>
								<li class="fragment">The <span style="color: blue">Lisp</span> programming language was created in this period</li>
								<li class="fragment">A. Newell and H. Simon developed the <span style="color: red">General Problem Solver</span> having the goal of simulating human reasoning in the resolution of simple problems</li>
								<li class="fragment">Time Sharing (DEC)</li>
								<li class="fragment">Microworlds (problems solved in a controlled environment)</li>
								<ul>
									<li class="fragment">Most notable example: Blocks world</li>
								</ul>
							</ul>
						</ul>
					</section>
					<section>
						<h1>"Zipped" history of AI</h1>
						<ul>
							<li class="fragment">1966 - 1974: A dose of reality</li>
							<ul>
								<li class="fragment">In 1958, H. Simon predicted that within 10 years, a computer would be chess champion and a significant mathematical theorem would be proved by a computer!!!</li>
								<li class="fragment">However...</li>
								<ul>
									<li class="fragment">Programs knew nothing about their subject matter; They succeeded by simple syntactic manipulation</li>
									<ul>
										<li class="fragment">Example: by simple syntactic manipulation, the sentence “The spirit is willing but the flesh is weak” was translated as  “The vodka is good but the meat is rotten”...</li>
									</ul>
									<li class="fragment">The methods able to solve simple problems were not able to solve general and difficult problems</li>
								</ul>
								<li class="fragment">Computers had not enough computational power to solve “larger” problems</li>
								<li class="fragment">Difficulties due to computational limitations - genetic algorithms (“good” idea, but impossible to verify with existent hardware)</li>
								<li class="fragment">“Perceptrons” book (Minsky and Papert, 1969) that, among other things, address perceptron limitations</li>
								<li class="fragment">Strong decrease in AI research funding (the so called “AI Winter”)</li>
								<li class="fragment"><span style="color:red">(Only) in 1997 a computer beaten the human chess world champion</span></li>
								
							</ul>
						</ul>
					</section>
					<section>
						<h1>"Zipped" history of AI</h1>
						<ul>
							<li class="fragment">1969 - 1979: Knowledge based systems</li>
							<ul>
								<li class="fragment">It was realized the necessity of using knowledge about the problem to solve in order to achieve better results</li>
								<li class="fragment"><span style="color:blue">Expert systems</span>: systems intended to simulate the reasoning of an expert in a given area</li>
								<ul>
									<li class="fragment">Examples: Tendril (molecular structures identification), MyCin (bacteria identification)</li>
								</ul>
								<li class="fragment">Expert systems led to a strong effort in the development of knowledge representation methods</li>
								<li class="fragment">Creation of the <span style="color:red">Prolog</span> programming language</li>
								
							</ul>
						</ul>
					</section>
					<section>
						<h1>"Zipped" history of AI</h1>
						<ul>
							<li class="fragment">1980 - Present: AI becomes an industry</li>
							<ul>
								<li class="fragment">AI starts to pay: expert systems, robotics, vision systems</li>
							</ul>
							<li class="fragment">1986 - Present: The return of neural networks</li>
							<ul>
								<li class="fragment">It started mainly due to the Back-propagation algorithm</li>
							</ul>
							<li class="fragment">1987 - Present: AI adopts the scientific method</li>
							<ul>
								<li class="fragment"><span style="color:blue">It is now more common to build on existing theories than to propose brand new ones</span>, <span style="color:red">to base claims on rigorous theorems or hard experimental evidence rather than on intuition</span>, <span style="color:green">and to show relevance to real-world applications rather than toy examples</span></li>
							</ul>							
						</ul>
					</section>
				</section>							
				<section>
					<h1>Humans are inspired by nature</h1>
					<ul>
							<li class="fragment">Birds</li>
							<ul>
								<li class="fragment">Flying - Airplanes</li>
							</ul>
							<li class="fragment">Horses</li>
							<ul>
								<li class="fragment">Cars</li>
							</ul>
							<li class="fragment">Bio-Inspired Computing</li>
							<ul>
								<li class="fragment">Evolutionary Algorithms - Evolution</li>
								<li class="fragment">Cellular Automata - Life</li>
								<li class="fragment">Ant Colony Optimization - Ants</li>
								<li class="fragment">Particle Swarm Optimization - Birds Flocking</li>
								<li class="fragment">Artificial Neural Networks - Biological Neural Networks</li>
								<li class="fragment">...</li>
							</ul>							
						</ul>
				</section>
				<section>
					<section>
						<h1>The Brain</h1>
					</section>
					<section>
						<img src="NNs/biologicalNeuron.png" class="plain" width="40%">
						<p class="fragment">Neurons are the fundamental unit of the nervous system tissue</p>
						<p class="fragment">A biologic neuron has three types of components that are specially interesting for the the understanding of artificial neurons:</p>

						<ul>
							<li class="fragment">Dendrites</li>
							<li class="fragment">Soma</li>
							<li class="fragment">Axon</li>
						</ul>
					</section>
					<section>
						<img src="NNs/biologicalNeuron.png" class="plain" width="40%">
						<p class="fragment">Each neuron is composed of a cellular body with several branches called dendrites and an isolated longer branch, called axon</p>
						<p class="fragment">Dendrites connect to other neurons’ axons through junctions called synapses</p>
						<p class="fragment">A neuron can be connected to hundreds of thousands of other neurons</p>
					</section>
					<section>
						<img src="NNs/biologicalNeuron.png" class="plain" width="40%">
						<p class="fragment">Signals propagate between neurons through a complicated electrochemical reaction which leads synapses to produce chemical substances that enter through dendrites.</p>
						<p class="fragment">This can raise or diminish the electrical potential of the cellular body</p>
						<p class="fragment">If the electrical potential overpasses some limit, an electrical impulse is sent to the axon, which spreads by its ramifications, thus transmitting electrical signals to other neurons</p>
					</section>
					<section>
						<img src="NNs/biologicalNeuron.png" class="plain" width="40%">
						<p class="fragment">An average brain has something on the order of 100 billion neurons. Each neuron is connected to up to 10,000 other neurons, which means that the number of synapses is between 100 trillion and 1,000 trillion</p>
						<p class="fragment">It has been observed that often used connections become stronger and that neurons sometimes form new connections with other neurons. It is thought that <b>these mechanisms lead to learning</b></p>
						<p class="fragment">Computation is strongly parallel and asynchronous</p>
					</section>
				</section>
				<section>
					<section><h1>Artificial Neural Networks</h1></section>
					<section>
						<p class="fragment">Although artificial neurons and perceptrons were inspired by the biological processes scientists were able to observe in the brain back in the 50s, they do differ from their biological counterparts in several ways.</p>						
						<div class="fragment"><img src="NNs/analogy.png" class="plain"></div>
						<p class="fragment">The idea behind perceptrons (the predecessors to artificial neurons) is that it is possible to mimic certain parts of neurons, such as dendrites, cell bodies and axons using simplified mathematical models of what limited knowledge we have on their inner workings: signals can be received from dendrites, and sent down the axon once enough signals were received.</p>
					</section>					
					<section>
						<img src="NNs/analogy.png" class="plain" width="30%">
						<p class="fragment"> This outgoing signal can then be used as another input for other neurons, repeating the process.</p>
						<p class="fragment">Some signals are more important than others and can trigger some neurons to fire easier.</p>
						<p class = "fragment">Connections can become stronger or weaker, new connections can appear while others can cease to exist.</p>
						<p class = "fragment">We can mimic most of this process by coming up with a function that receives a list of weighted input signals and outputs some kind of signal if the sum of these weighted inputs reach a certain bias.</p>
						<p class = "fragment">Note that this simplified model does not mimic neither the creation nor the destruction of connections (dendrites or axons) between neurons, and ignores signal timing.</p>
						<p class = "fragment">However, this restricted model alone is powerful enough to work with simple classification tasks.</p>
					</section>
					<section>	
						<p>Invented by Frank Rosenblatt, the perceptron was originally intended to be a custom-built mechanical hardware instead of a software function. The Mark 1 perceptron was a machine built for image recognition tasks by the US navy.</p>		
						<iframe width="937" height="703" src="https://www.youtube.com/embed/cNxadbrN_aI" title="Perceptron Research from the 50&#39;s &amp; 60&#39;s, clip" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
					</section>
					<section>
						<p class="fragment">Just imagine the possibilities!</p>
						<p class="fragment">A machine that can mimic learning from experience with its neuron-like mind?</p>
						<p class="fragment">A machine that learns from examples it "sees" instead of scientists in glasses having to give it a set of hard coded instructions to work?</p>
						<p class="fragment">The hype was real, and people were optimistic.</p>
						<p class="fragment">Due to its resemblance to the biological neuron, and how promising perceptron networks initially were, the New York Times has reported in 1958 that <em>"the Navy [has] revealed the embryo of an electronic computer today that it expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence."</em></p>
					</section>					
		            <section>
		                <h3>Evolution of Neural Network Architectures</h3>
		                <p>Addressing the limitations of single-layer perceptrons in solving non-linear problems.</p>
		                <ul>
		                    <li class="fragment"><strong>Limitations of Single-Layer Perceptrons:</strong> Unable to solve non-linear problems like the XOR function.</li>
		                    <li class="fragment"><strong>Need for Multi-Layered Approaches:</strong> Complex data relationships require multiple layers, including hidden layers.</li>
		                    <li class="fragment"><strong>Training Challenges:</strong> Early training methods for multi-layer perceptrons were inefficient, typically involving random weight adjustments.</li>
		                    <li class="fragment"><strong>Innovative Breakthrough:</strong> Introduction of artificial neurons capable of using continuous values and activation functions facilitated effective training of multi-layer networks.</li>
		                    <li class="fragment"><strong>Impact on Model Capability:</strong> This change enabled the modeling of more complex relationships and improved the overall performance of neural networks.</li>
		                </ul>
		            </section>
		            <section>
		            	<img src="NNs/transferfunctions.png" class="plain" width="60%">
		            </section>
		            <section>
		                <h3>Transformative Advances in Neural Networks</h3>
		                <p>A small change in neuron modeling led to significant advancements in machine learning.</p>
		                <ul>
		                    <li class="fragment"><strong>Model Change:</strong> Shift from binary to continuous output values allowed neurons to function in complex mathematical formulas.</li>
		                    <li class="fragment"><strong>Backpropagation:</strong> This change enabled the use of backpropagation to efficiently train multiple layers.</li>
		                    <li class="fragment"><strong>Continuous vs. Binary:</strong> Unlike biological neurons that fire binary signals, artificial neurons send continuous signals.</li>
		                    <li class="fragment"><strong>Terminology Clarification:</strong> "Multilayer perceptron" is a misnomer as these actually use layers of artificial neurons.</li>
		                    <li class="fragment"><strong>Computational Costs:</strong> Initially, high computational costs limited their use until advances in computing power and data availability.</li>
		                    <li class="fragment"><strong>Revival with AlexNet:</strong> In 2012, AlexNet significantly advanced neural networks by winning the ImageNet challenge without relying on handcrafted features.</li>
		                </ul>
		            </section>
		            <section>
		            	<img src="NNs/alexnet.png" class="plain" width="100%">
		            </section>
					<section>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
					</section>
					<section>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
					</section>
					<section>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
					</section>
					<section>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
					</section>
					<section>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
					</section>
					<section>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
					</section>
					<section>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
					</section>
					<section>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
					</section>
					<section>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
						<p class="fragment"></p>
					</section>

				</section>

			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				width: 1920,
				height: 1080,
				controls: true,
				progress: true,
				center: true,
				hash: true,
				transition: 'convex',
				math: {
					// mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
					config: 'TeX-AMS_HTML-full',
					TeX: {
						Macros: {
							R: '\\mathbb{R}',
							set: [ '\\left\\{#1 \\; ; \\; #2\\right\\}', 2 ]
						}
					}
				},
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },					
					{ src: 'plugin/highlight/highlight.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/math/math.js', async: true }
				]
			});
		</script>
	</body>
</html>
